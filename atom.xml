<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/chenmingjian.github.io/atom.xml" rel="self"/>
  
  <link href="https://github.com/chenmingjian/chenmingjian.github.io/"/>
  <updated>2020-06-22T06:51:34.071Z</updated>
  <id>https://github.com/chenmingjian/chenmingjian.github.io/</id>
  
  <author>
    <name>chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Combining_detection_and_tracking_for_human_pose_estimation_in_videos</title>
    <link href="https://github.com/chenmingjian/chenmingjian.github.io/2020/06/08/Combining-detection-and-tracking-for-human-pose-estimation-in-videos/"/>
    <id>https://github.com/chenmingjian/chenmingjian.github.io/2020/06/08/Combining-detection-and-tracking-for-human-pose-estimation-in-videos/</id>
    <published>2020-06-08T12:36:24.000Z</published>
    <updated>2020-06-22T06:51:34.071Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 22 2020 14:57:23 GMT+0800 (GMT+08:00) --><p>让Top-down的视频追踪的效果不依赖于目标检测器， 是PoseTrack 2017 and 2018 datasets的SOTA。</p><h2 id="1-思路"><a class="markdownIt-Anchor" href="#1-思路"></a> 1. 思路</h2><p>通过在时间方向上人体区域的前后传播，并在前后帧的有人区域检测pose。<br>如果有的帧目标检测器效果不好，因为相邻帧间的人体空间位移通常不大，所以可以使用前后帧的结果。<br>算法组成：</p><ol><li>剪切追踪网络（Clip Tracking Network）对小视频片段同时进行身体关节检测和跟踪;</li><li>视频跟踪，把由剪切追踪网络产生的定长轨迹整合到任意长度的轨迹;</li><li>空间时间合并过程，基于空间和时间平滑项来细化关节位置。</li></ol><h3 id="11-贡献"><a class="markdownIt-Anchor" href="#11-贡献"></a> 1.1 贡献</h3><ol><li>Clip Tracking Network</li><li>Video Tracking Pipeline</li><li>SpatialTemporal merging of pose hypotheses</li></ol><h3 id="12解决了什么问题"><a class="markdownIt-Anchor" href="#12解决了什么问题"></a> 1.2解决了什么问题？</h3><p>视频人体姿态估计添加时间和空间上的平滑。</p><h3 id="13还存在什么问题"><a class="markdownIt-Anchor" href="#13还存在什么问题"></a> 1.3还存在什么问题？</h3><p>同一帧的人反复预测。计算量巨高。</p><h2 id="2-疑惑"><a class="markdownIt-Anchor" href="#2-疑惑"></a> 2. 疑惑</h2><h2 id="3-note"><a class="markdownIt-Anchor" href="#3-note"></a> 3. note</h2><h2 id="todo"><a class="markdownIt-Anchor" href="#todo"></a> TODO</h2><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 22 2020 14:57:23 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;让Top-down的视频追踪的效果不依赖于目标检测器， 是PoseTrack 2017 and 2018 datasets的SOTA。&lt;/p&gt;&lt;
      
    
    </summary>
    
    
      <category term="Paper笔记" scheme="https://github.com/chenmingjian/chenmingjian.github.io/categories/Paper%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Multi-person Pose Estimation in the Wild</title>
    <link href="https://github.com/chenmingjian/chenmingjian.github.io/2020/06/03/Multi-person-Pose-Estimation-in-the-Wild/"/>
    <id>https://github.com/chenmingjian/chenmingjian.github.io/2020/06/03/Multi-person-Pose-Estimation-in-the-Wild/</id>
    <published>2020-06-03T02:00:45.737Z</published>
    <updated>2020-05-23T01:27:10.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 22 2020 14:57:23 GMT+0800 (GMT+08:00) --><p>​ECCV2017；google；heatmap+offset</p><h2 id="思路"><a class="markdownIt-Anchor" href="#思路"></a> 思路</h2><h3 id="贡献"><a class="markdownIt-Anchor" href="#贡献"></a> 贡献</h3><ul><li>heatmap+offsets<ul><li>两者结合与CVPR2020的<a href="https://arxiv.org/abs/1911.07524" target="_blank" rel="noopener"><em>The Devil</em></a>中提到的无偏估计方法相同，待后续对比。</li><li>分解为两个问题（二分类以及offset回归），那么两个问题需要的feature map的层次是否一致？</li></ul></li><li>keypoint-based NMS</li><li>keypoint-based confidence score estimation, instead of box-level scoring.</li></ul><h3 id="解决了什么问题"><a class="markdownIt-Anchor" href="#解决了什么问题"></a> 解决了什么问题？</h3><ul><li>反驳了一些论文觉得bottom-up方法精度更高的观点。<ul><li>在今天看来top-down均优于bottom-up方法<ul><li>子任务分工明确，单个任务更简单。</li><li>在一定意义上规避了多尺度问题。</li></ul></li></ul></li></ul><h3 id="还存在什么问题"><a class="markdownIt-Anchor" href="#还存在什么问题"></a> 还存在什么问题？</h3><p>​pass</p><h2 id="疑惑"><a class="markdownIt-Anchor" href="#疑惑"></a> 疑惑</h2><ul><li>3.1中使用atrous convolution如何生成更密集的feature map？</li><li>文中提到不使用回归坐标点的方法是因为无法处理在图片中多人的关键点。</li><li>heatmap的loss函数中似乎没有使用负样本。</li></ul><h2 id="note"><a class="markdownIt-Anchor" href="#note"></a> note</h2><ol><li>COCO test-dev 0.649; COCO test 0.643。</li><li>目标检测部分，没有使用multi-scale evaluation。</li><li>loss函数中heatmap与offset的比例是4：1，是否可以学习？如果不能学习是否可以设置与上一步Loss的比例成反比，动态损失函数。</li></ol><h2 id="todo"><a class="markdownIt-Anchor" href="#todo"></a> TODO</h2><ul><li>探究为什么top-down的结果要更好。</li><li>unpooling是可以做到unbias。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 22 2020 14:57:23 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;​	ECCV2017；google；heatmap+offset&lt;/p&gt;&lt;h2 id=&quot;思路&quot;&gt;&lt;a class=&quot;markdownIt-Anc
      
    
    </summary>
    
    
      <category term="Paper笔记" scheme="https://github.com/chenmingjian/chenmingjian.github.io/categories/Paper%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>EfficentPose</title>
    <link href="https://github.com/chenmingjian/chenmingjian.github.io/2020/05/23/EfficientPose/"/>
    <id>https://github.com/chenmingjian/chenmingjian.github.io/2020/05/23/EfficientPose/</id>
    <published>2020-05-23T05:19:59.000Z</published>
    <updated>2020-05-23T07:36:52.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jun 22 2020 14:57:23 GMT+0800 (GMT+08:00) --><h2 id="1-思路"><a class="markdownIt-Anchor" href="#1-思路"></a> 1. 思路</h2><h3 id="11-贡献"><a class="markdownIt-Anchor" href="#11-贡献"></a> 1.1 贡献</h3><h3 id="12解决了什么问题"><a class="markdownIt-Anchor" href="#12解决了什么问题"></a> 1.2解决了什么问题？</h3><h3 id="13还存在什么问题"><a class="markdownIt-Anchor" href="#13还存在什么问题"></a> 1.3还存在什么问题？</h3><h2 id="2-疑惑"><a class="markdownIt-Anchor" href="#2-疑惑"></a> 2. 疑惑</h2><h2 id="3-note"><a class="markdownIt-Anchor" href="#3-note"></a> 3. note</h2><h2 id="todo"><a class="markdownIt-Anchor" href="#todo"></a> TODO</h2><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Jun 22 2020 14:57:23 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;1-思路&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-思路&quot;&gt;&lt;/a&gt; 1. 思路&lt;/h2&gt;&lt;h3 
      
    
    </summary>
    
    
      <category term="Paper笔记" scheme="https://github.com/chenmingjian/chenmingjian.github.io/categories/Paper%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
